# -*- coding: utf-8 -*-
"""Groq RAG App with Gradio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ESjbA-kzKyDNow92V8NjKiDTQFokIr5r

### RETRIEVAL AUGMENTED GENERATION [RAG]

### What is RAG?
One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as retrieval-augmented generation (RAG). RAG is a technique for augmenting LLM knowledge with additional data, which can be your own data.

LLMs can reason about wide-ranging topics, but their knowledge is limited to public data up to the specific point in time that they were trained. If you want to build AI applications that can reason about private data or data introduced after a modelâ€™s cut-off date, you must augment the knowledge of the model with the specific information that it needs. The process of bringing and inserting the appropriate information into the model prompt is known as RAG.

LangChain has several components that are designed to help build Q&A applications and RAG applications, more generally.

### RAG architecture
A typical RAG application has two main components:

* **Indexing**: A pipeline for ingesting and indexing data from a source. This usually happens offline.

* **Retrieval and generation**: The actual RAG chain takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.

### Importing Required Libraries
"""

!pip install -U langchain-community
!pip install langchain-groq
!pip install wget

# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain_groq import ChatGroq

import wget

"""### Preprocessing

Loading the document
"""

file_name = 'The_Adventures_of_Sherlock_Holmes.txt'
url = 'https://www.gutenberg.org/ebooks/1661.txt.utf-8'

wget.download(url, out=file_name)
print('Book Downloaded')

with open(file_name, 'r') as file:
  content = file.read()
  print(content[:500])

"""### Splitting Document into Chunks"""

loader = TextLoader(file_name)
document = loader.load()
text_splitter = CharacterTextSplitter(chunk_size = 1000, chunk_overlap = 30)
texts = text_splitter.split_documents(document)
print(len(texts))

"""### Embedding and storing
This step is the `embed` and `store` processes in `Indexing`.
"""

!pip install chromadb
embeddings = HuggingFaceEmbeddings()
docsearch = Chroma.from_documents(texts, embeddings)
print('document ingested')

model_id = 'llama-3.1-8b-instant'

llama_llm = ChatGroq(
    api_key= "API-KEY",
    model_name=model_id,
    temperature=0.5,
    max_tokens=256
)

"""### RetrievalQA"""

qa = RetrievalQA.from_chain_type(llm = llama_llm,
                                 chain_type = 'stuff',
                                 retriever = docsearch.as_retriever(),
                                 return_source_documents = False
                                 )
query = 'Can you summarize the document?'
qa.invoke(query)

qa = RetrievalQA.from_chain_type(llm = llama_llm,
                                 chain_type = 'stuff',
                                 retriever = docsearch.as_retriever(),
                                 return_source_documents = False
                                 )
query = 'Give me the important Charater names?'
qa.invoke(query)

qa = RetrievalQA.from_chain_type(llm = llama_llm,
                                 chain_type = 'stuff',
                                 retriever = docsearch.as_retriever(),
                                 return_source_documents = False
                                 )
query = 'Is there a movie based on the book?'
qa.invoke(query)

"""### As you can see, the query is asking something that does not exist in the document. The LLM responds with information that actually is not true. You don't want this to happen, so you must add a prompt to the LLM.

### Using Prompt Template
"""

# 1. Define custom prompt template
prompt_template = '''Use the information from the document to answer the question at the end.
If you don't know the answer,
just say that you don't know, definately do not try to make up an answer.

{context}

Question: {question}
'''

# 2. Create a PromptTemplate object from the string
prompt = PromptTemplate(template=prompt_template,
                        input_variables=['context', 'question']) # Note: 'question' is standard

# 3. Create the dictionary to pass the custom prompt
chain_type_kwargs = {'prompt': prompt}

# 4. Use the dictionary when initializing your RetrievalQA chain
qa = RetrievalQA.from_chain_type(llm = llama_llm,
                                 chain_type = 'stuff',
                                 retriever = docsearch.as_retriever(),
                                 chain_type_kwargs = chain_type_kwargs,
                                 return_source_documents = False
                                 )
query = 'Is there a movie based on the book?'
qa.invoke(query)

"""From the answer, you can see that the model responds with "don't know".

"""

# --- Gradio Application Functions ---
# These functions will be called by the Gradio interface.
def answer_question(query):
    """
    This function processes the user's query and returns an answer.
    """
    if not query:
        return "Please enter a question."

    try:
        response = qa.invoke({'query': query})
        return response['result']
    except Exception as e:
        return f"An error occurred: {e}"

# --- Gradio Interface ---
# Define the Gradio web interface
import gradio as gr
rag_application = gr.Interface(
    fn=answer_question,
    allow_flagging='never',
    inputs=gr.Textbox(label='Input Query', lines=5, placeholder="Enter your question here"),
    outputs=gr.Textbox(label="Answer"),
    title="Groq-powered RAG Chatbot for Sherlock Holmes",
    description="Ask a question and get an answer based on 'The Adventures of Sherlock Holmes'.",
)

if __name__ == "__main__":
    rag_application.launch()

